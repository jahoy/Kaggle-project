{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instacart Modeling\n",
    "\n",
    "1. Merge Every work - data cleaning, feature enginering, word2vec\n",
    "\n",
    "2. Modeling - LightGBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data cleaning work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "#data cleaning\n",
    "train_orders = pd.read_csv('data/order_products__train.csv', dtype={'order_id': np.uint32,\n",
    "           'product_id': np.uint16, 'reordered': np.int8, 'add_to_cart_order': np.uint8 })\n",
    "orders = pd.read_csv('data/orders.csv', dtype={'order_hour_of_day': np.uint8,\n",
    "           'order_number': np.uint8, 'order_id': np.uint32, 'user_id': np.uint32,\n",
    "           'order_dow': np.uint8, 'days_since_prior_order': np.float16})\n",
    "\n",
    "orders.eval_set = orders.eval_set.replace({'prior': 0, 'train': 1, 'test':2}).astype(np.uint8)\n",
    "orders.days_since_prior_order = orders.days_since_prior_order.fillna(30).astype(np.uint8)\n",
    "\n",
    "train_orders = train_orders.merge(orders[['user_id','order_id']], left_on = 'order_id', right_on = 'order_id', how = 'inner')\n",
    "train_orders = train_orders[train_orders['reordered']==1].drop('reordered',axis=1)\n",
    "orders.set_index('order_id', drop=False, inplace=True)\n",
    "train1=orders[['order_id','eval_set']].loc[orders['eval_set']==1]\n",
    "train1['actual'] = train_orders.groupby('order_id').aggregate({'product_id':lambda x: list(x)})\n",
    "train1['actual']=train1['actual'].fillna('')\n",
    "n_actual = train1['actual'].apply(lambda x: len(x)).mean()   # this is the average cart size\n",
    "\n",
    "test1=orders[['order_id','eval_set']].loc[orders['eval_set']==2]\n",
    "test1['actual']=' '\n",
    "traintest1=pd.concat([train1,test1])\n",
    "traintest1.set_index('order_id', drop=False, inplace=True)\n",
    "\n",
    "del orders, train1, test1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import feature engieering file - instacart_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import feature engieering file - instacart_data.csv\n",
    "data = pd.read_csv(\"data/instacart_data.csv\")\n",
    "data = data.astype(dtype= {'user_id' : np.uint32, 'product_id'  : np.uint16,\n",
    "            'up_orders'  : np.uint8, 'up_first_order' : np.uint8, 'up_last_order' : np.uint8,\n",
    "            'up_average_cart_position' : np.uint8, 'prod_orders' : np.uint16, \n",
    "            'prod_reorder_probability' : np.float16,   \n",
    "            'prod_reorder_ratio' : np.float16, 'user_orders' : np.uint8,\n",
    "            'user_period' : np.uint8, 'user_mean_days_since_prior' : np.uint8,\n",
    "            'user_total_products' : np.uint8, 'user_reorder_ratio' : np.float16, \n",
    "            'user_distinct_products' : np.uint8, 'user_average_basket' : np.uint8,\n",
    "            'order_id'  : np.uint32, 'eval_set' : np.uint8, \n",
    "            'days_since_prior_order' : np.uint8, 'up_order_rate' : np.float16, \n",
    "            'up_orders_since_last_order':np.uint8,\n",
    "            'aisle_id': np.uint8, 'department_id': np.uint8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import word2vec file - pca_w2v.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import word2vec file - pca_w2v.csv\n",
    "word2vec = pd.read_csv(\"data/pca_w2v.csv\")\n",
    "word2vec = word2vec.astype(dtype= {'product_id'  : np.uint16,\n",
    "                                  \"0\": np.float16, \"1\": np.float16, \"2\": np.float16,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(word2vec, on = \"product_id\", how = \"left\")\n",
    "data['reordered']=data['reordered'].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test 셋 칼럼수 맞쳐줌\n",
    "train = data[data['eval_set'] == 1].drop(['eval_set', 'user_id', 'product_id', 'order_id'], axis = 1)\n",
    "test =  data[data['eval_set'] == 2].drop(['eval_set', 'user_id', 'reordered'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1점수로 채점,   F1점수는 최적화 시킬수 있음,  최적화시키기 위해 미리 만들어둠\n",
    "check =  data.drop(['eval_set', 'user_id', 'reordered'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM 사용하는 또 다른 방식: 학습할때 train함수를 이용해서 학습시키는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "import gc\n",
    "gc.collect()\n",
    "import lightgbm as lgb\n",
    "print('formatting and training LightGBM ...')\n",
    "\n",
    "\n",
    "lgb_train = lgb.Dataset( train[train.columns.difference(['reordered'])], label=train['reordered'])\n",
    "\n",
    "#'feature_fraction' ~ colsample_bytree\n",
    "# 'bagging_fraction' ~ subsample\n",
    "# 'num_iterations' ~ n_estimators\n",
    "# 'min_data_in_leaf' ~ min_child_samples : 하나 잎사귀에 들어가는 최소 샘플의 갯수\n",
    "# 'max_bin' ~ 숫자형 데이터를 binning을 해주는 overfitting을 막는것\n",
    "# 'binary_ logloss ' ~ 각각의 제품마다 확률값, 클래스가 0또는 1\n",
    "params = {'task': 'train', 'boosting_type': 'gbdt',   'objective': 'binary', 'metric': {'binary_logloss', 'auc'},\n",
    "    'num_iterations' : 130, 'max_bin' : 100, 'num_leaves': 512, 'feature_fraction': 0.8,  'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 5, 'min_data_in_leaf' : 200, 'learning_rate' : 0.05}\n",
    "\n",
    "# set lower num_boost_round (I used 300 instead of 25 at home) to avoid time-out on Kaggle\n",
    "\n",
    "# num_boost_round ~ 몇번 모델을 돌릴것인가\n",
    "lgb_model = lgb.train(params, lgb_train, num_boost_round = 130, \n",
    "                      #valid_sets = lgb_eval, early_stopping_rounds=15\n",
    "                     )\n",
    "\n",
    "#F1 점수 최적으로 올리는 코드\n",
    "del lgb_train# X_train, y_train\n",
    "gc.collect()\n",
    "def combi(z,df):\n",
    "    \n",
    "    prd_bag = dict()\n",
    "    z_bag = dict()\n",
    "    for row in df.itertuples():\n",
    "        if row.reordered > z:   \n",
    "            try:\n",
    "                prd_bag[row.order_id] += ' ' + str(row.product_id)\n",
    "                z_bag[row.order_id]+= ' ' + str(int(100*row.reordered))\n",
    "            except:\n",
    "                prd_bag[row.order_id] = str(row.product_id)\n",
    "                z_bag[row.order_id]= str(int(100*row.reordered))\n",
    "\n",
    "    for order in df.order_id:\n",
    "        if order not in prd_bag:\n",
    "            prd_bag[order] = ' '\n",
    "            z_bag[order] = ' '\n",
    "\n",
    "    return prd_bag,z_bag \n",
    "\n",
    "# F1 function uses the actual products as a list in the train set and the list of predicted products\n",
    "\n",
    "def f1_score_single(x):                \n",
    "\n",
    "    y_true = x.actual\n",
    "    y_pred = x.list_prod\n",
    "    if y_true == '' and y_pred ==[] : return 1.\n",
    "    y_true = set(y_true)\n",
    "    y_pred = set(y_pred)\n",
    "    cross_size = len(y_true & y_pred)\n",
    "    if cross_size == 0: return 0.\n",
    "    p = 1. * cross_size / len(y_pred)\n",
    "    r = 1. * cross_size / len(y_true)\n",
    "    return 2 * p * r / (p + r)\n",
    "# check feature importance\n",
    "#lgb.plot_importance(lgb_model, figsize=(7,9))\n",
    "#plt.show()\n",
    "print(' Applying model to all data - both train and test ')\n",
    "\n",
    "check['reordered'] = lgb_model.predict(check[check.columns.difference(\n",
    "    ['order_id', 'product_id'])], num_iteration = lgb_model.best_iteration)\n",
    "\n",
    "gc.collect()\n",
    "print(' summarizing products and probabilities ...')\n",
    "\n",
    "# get the prediction for a range of thresholds\n",
    "\n",
    "tt=traintest1.copy()\n",
    "i=0\n",
    "\n",
    "for z in [0.17, 0.21, 0.25]:\n",
    "    \n",
    "    prd_bag,z_bag = combi(z,check)\n",
    "    ptemp = pd.DataFrame.from_dict(prd_bag, orient='index')\n",
    "    ptemp.reset_index(inplace=True)\n",
    "    ztemp = pd.DataFrame.from_dict(z_bag, orient='index')\n",
    "    ztemp.reset_index(inplace=True)\n",
    "    ptemp.columns = ['order_id', 'products']\n",
    "    ztemp.columns = ['order_id', 'zs']\n",
    "    ptemp['list_prod'] = ptemp['products'].apply(lambda x: list(map(int, x.split())))\n",
    "    ztemp['list_z'] = ztemp['zs'].apply(lambda x: list(map(int, x.split())))\n",
    "    n_cart = ptemp['products'].apply(lambda x: len(x.split())).mean()\n",
    "    tt = tt.merge(ptemp,on='order_id',how='inner')\n",
    "    tt = tt.merge(ztemp,on='order_id',how='inner')\n",
    "    tt.drop(['products','zs'],axis=1,inplace=True)\n",
    "    tt['zavg'] = tt['list_z'].apply(lambda x: 0.01*np.mean(x) if x!=[] else 0.).astype(np.float16)\n",
    "    tt['zmax'] = tt['list_z'].apply(lambda x: 0.01*np.max(x) if x!=[] else 0.).astype(np.float16)\n",
    "    tt['zmin'] = tt['list_z'].apply(lambda x: 0.01*np.min(x) if x!=[] else 0.).astype(np.float16)\n",
    "    tt['f1']=tt.apply(f1_score_single,axis=1).astype(np.float16)\n",
    "    F1 = tt['f1'].loc[tt['eval_set']==1].mean()\n",
    "    tt = tt.rename(columns={'list_prod': 'prod'+str(i), 'f1': 'f1'+str(i), 'list_z': 'z'+str(i),\n",
    "                'zavg': 'zavg'+str(i), 'zmax': 'zmax'+str(i),  'zmin': 'zmin'+str(i)})\n",
    "    print(' z,F1,n_actual,n_cart :  ', z,F1,n_actual,n_cart)\n",
    "    i=i+1\n",
    "\n",
    "tt['fm'] = tt[['f10', 'f11', 'f12']].idxmax(axis=1)\n",
    "tt['f1'] = tt[['f10', 'f11', 'f12']].max(axis=1)\n",
    "tt['fm'] = tt.fm.replace({'f10': 0,'f11': 1, 'f12':2}).astype(np.uint8)\n",
    "print(' f1 maximized ', tt['f1'].loc[tt['eval_set']==1].mean())\n",
    "    \n",
    "#del prd_bag, z_bag, ptemp, ztemp\n",
    "gc.collect()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Fitting the second classifier for F1 ...')\n",
    "\n",
    "X=tt[[ 'zavg0', 'zmax0','zmin0', 'zavg1', 'zmax1', 'zmin1', 'zavg2', 'zmax2', 'zmin2']].loc[tt['eval_set']==1]\n",
    "y=tt['fm'].loc[tt['eval_set']==1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "clf = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "print('GB Accuracy on training set: {:.2f}' .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy on test set: {:.2f}' .format(clf.score(X_test, y_test)))\n",
    "#pd.DataFrame(clf.feature_importances_, index=X_train.columns, columns=[\"Importance\"]).plot(kind='bar')\n",
    "#plt.show()\n",
    "\n",
    "final=tt[['order_id','prod0','prod1','prod2','zavg0']].loc[tt['eval_set']==2]\n",
    "df_test=tt[[ 'zavg0', 'zmax0','zmin0', 'zavg1', 'zmax1', 'zmin1', 'zavg2', 'zmax2', 'zmin2']].loc[tt['eval_set']==2]\n",
    "final['fit']= clf.predict(df_test)\n",
    "final['best'] = final.apply(lambda row: row['prod0'] if row['fit']==0 else \n",
    "                                 ( row['prod1'] if row['fit']==1 else  row['prod2'] )  , axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def mylist(x):\n",
    "    prodids = x.best\n",
    "    zavg = x.zavg0\n",
    "    if prodids == []: return 'None'            \n",
    "    if zavg < 0.5:\n",
    "        if len(prodids) == 1: return  str(prodids[0])+' None'\n",
    "        if len(prodids) == 2: return  str(prodids[0])+ ' '+ str(prodids[1]) +' None'\n",
    "    return ' '.join(str(i) for i in prodids)\n",
    "\n",
    "final['products']=final.apply(mylist,axis=1)\n",
    "\n",
    "final[['order_id','products']].to_csv('final_submission1.csv', index=False)  \n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
