{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SanFranciso Crime Classification\n",
    "\n",
    "Dataset: [kaggle](https://www.kaggle.com/c/sf-crime/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "train = pd.read_csv('../input/train.csv', parse_dates=['Dates'])\n",
    "test = pd.read_csv('../input/test.csv', parse_dates=['Dates'], index_col='Id')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#날짜를 칼럼별로 나눔\n",
    "train['Date'] = pd.to_datetime(train['Dates'].dt.date)\n",
    "train['n_days'] = (train['Date'] - train['Date'].min()).apply(lambda x: x.days)\n",
    "train['Day'] = train['Dates'].dt.day\n",
    "train['DayOfWeek'] = train['Dates'].dt.weekday\n",
    "train['Month'] = train['Dates'].dt.month\n",
    "train['Year'] = train['Dates'].dt.year\n",
    "train['Hour'] = train['Dates'].dt.hour\n",
    "train['Minute'] = train['Dates'].dt.minute\n",
    "train['Block'] = train['Address'].str.contains('block', case=False).apply(lambda x: 1 if x == True else 0)\n",
    "train['ST'] = train['Address'].str.contains('ST', case=False).apply(lambda x: 1 if x == True else 0)\n",
    "train[\"X_Y\"] = train[\"X\"] - train[\"Y\"]\n",
    "train[\"XY\"] = train[\"X\"] + train[\"Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Date'] = pd.to_datetime(test['Dates'].dt.date)\n",
    "test['n_days'] = (test['Date'] - test['Date'].min()).apply(lambda x: x.days)\n",
    "test['Day'] = test['Dates'].dt.day\n",
    "test['DayOfWeek'] = test['Dates'].dt.weekday\n",
    "test['Month'] = test['Dates'].dt.month\n",
    "test['Year'] = test['Dates'].dt.year\n",
    "test['Hour'] = test['Dates'].dt.hour\n",
    "test['Minute'] = test['Dates'].dt.minute\n",
    "test['Block'] = test['Address'].str.contains('block', case=False).apply(lambda x: 1 if x == True else 0)\n",
    "test['ST'] = test['Address'].str.contains('ST', case=False).apply(lambda x: 1 if x == True else 0)\n",
    "test[\"X_Y\"] = test[\"X\"] - test[\"Y\"]\n",
    "test[\"XY\"] = test[\"X\"] + test[\"Y\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자를 숫자로 바꾼다. 머신러닝 모델에 넣을수 있기위해\n",
    "#1 라이브러리 import \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#2 객체를 불러온다\n",
    "le1 = LabelEncoder()\n",
    "\n",
    "#3 lookuptable 보고 함수 실행\n",
    "train['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\n",
    "test['PdDistrict'] = le1.transform(test['PdDistrict'])\n",
    "\n",
    "le2 = LabelEncoder()\n",
    "y= le2.fit_transform(train['Category'])#종속 변수 분리 + 문자->숫자 변경\n",
    "\n",
    "#address 는 둘다 train, test 에 lookup table을 만든다.\n",
    "le3 = LabelEncoder()\n",
    "le3.fit(list(train['Address']) + list(test['Address']))\n",
    "# lookuptable 참조하여 transform\n",
    "train['Address'] = le3.transform(train['Address'])\n",
    "test['Address'] = le3.transform(test['Address'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#쓸데없는 칼럼 드랍\n",
    "\n",
    "train.drop(['Dates','Date','Descript','Resolution', 'Category'], 1, inplace=True)\n",
    "test.drop(['Dates','Date',], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier 모델 선정 기준 1. 데이터셋에 굉장히 많은 문자열을 포함하고있다. 2. 데이터가 어느정도 대용량이다.\n",
    "# LGBM 모델 선정 이유 1. 문자열 데이터를 잘 처리함 2. 데이터가 대용량 사용 가능(데이터가 크면 클수록 속도가 더 빨라지는 특징) 3. tree모델은 데이터가 분류가 되기만 한다면 학습을 잘함. classifier에 특화\n",
    "# 문자열 데이터를 잘 처리하는 모델: 1)Catboost(문자열 가장 잘 처리, 데이터 적은량에 쓴다 ,데이터가 많으면 시간이 많이 걸린다)\n",
    "# 문자열 데이터 다룰때 => 1. label encoding 을 해야겠다. 2. tree 모델을 서야겠다.(tree모델은 데이터가 분류가 되기만 한다면)\n",
    "\n",
    "#1. 모델을 불러온다: 분류 할때 classifier 쓴다\n",
    "from lightgbm import LGBMClassifier\n",
    "#2. 모델을 선언한다.\n",
    "model = LGBMClassifier(objective=\"multiclass\", num_class=39, max_bin = 465, max_delta_step = 0.9,\n",
    "                      learning_rate=0.4, num_leaves = 42, n_estimators=100,)\n",
    "#3. 모델을 학습시킨다. 누가 카데고리 feature인지 알려줌,/ address를 카데고리 feature에 안넣어진이유: 너무 많은 unique 갯수를 가지고 있음 => 학습시간이 길다., 성\n",
    "model.fit(train, y, categorical_feature=[\"PdDistrict\", \"DayOfWeek\"])\n",
    "#4. 모델을 예측한다. / classifer는 proba(확률)를 붙힘 - Log loss로 평가하기때문에 label로 예측해버리면 틀렸을때 패널티가 너무 크기때문에 Proba(확률)로 예측함\n",
    "preds = model.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le2.inverse_transform(np.linspace(0, 38, 39, dtype='int16')  => 역으로 숫자에서 문자로 바꿈\n",
    "submission = pd.DataFrame(preds, columns=le2.inverse_transform(np.linspace(0, 38, 39, dtype='int16')), index=test.index)\n",
    "submission.to_csv('LGBM_final.csv', index_label='Id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
